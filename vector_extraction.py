#!/usr/bin/env python3
"""
make_ft_vec.py
------------------------------------
è¤‡æ•°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚°ãƒ©ãƒ•ã‚’ã¾ã¨ã‚ã¦ä¸€åº¦ã«å‡¦ç†ã—ã€ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å„å…¥åŠ›ã‚°ãƒ©ãƒ•ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ä¿å­˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

ã€è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘
ä»¥ä¸‹ã®å¤‰æ•°ã‚’æ›¸ãæ›ãˆã‚‹ã“ã¨ã§ã€å…¥åŠ›ã‚°ãƒ©ãƒ•ã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã€FastTextãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆã€
åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚„ã‚¨ãƒãƒƒã‚¯æ•°ã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§æŒ‡å®šã§ãã¾ã™ã€‚
"""
import json
import os
import random
import glob
import numpy as np
from gensim.models import FastText
from tqdm import tqdm

# ==== è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ ====
# å…¥åŠ›ã‚°ãƒ©ãƒ•ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã‚’è¿½åŠ 
INPUT_GRAPH_DIRS = [
    "./programs/Projects/new_6/dataset/graphs",
    "./programs/Projects/Project6/dataset/graphs",
    # ãã®ä»–ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’ã“ã“ã«è¿½è¨˜
]
# å­¦ç¿’æ¸ˆ FastText ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹
MODEL_PATH = "fasttext_all.bin"
# FastText åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒæ•°
DIM = 64
# FastText å­¦ç¿’ã®ã‚¨ãƒãƒƒã‚¯æ•°
EPOCH = 20


def load_graph(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def graph_to_sentences(g, n_walks=10, walk_len=6):
    id2node = {n['id']: n for n in g['nodes']}
    sentences = [[n['label'], n.get('attributes', '')] for n in g['nodes']]
    adj = {}
    for e in g['edges']:
        adj.setdefault(e['source'], []).append(e['target'])
    for v in id2node:
        for _ in range(n_walks):
            cur, walk = v, []
            for _ in range(walk_len):
                walk.append(id2node[cur]['label'])
                if cur not in adj:
                    break
                cur = random.choice(adj[cur])  # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯
            sentences.append(walk)
    return sentences


def train_fasttext(sentences, dim, epoch):
    return FastText(
        vector_size=dim,
        window=5,
        min_count=1,
        sg=1,
        epochs=epoch,
        sentences=sentences
    )


def save_node_vectors(g, model, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    vecs = []
    for n in g['nodes']:
        toks = [n['label'], n.get('attributes', '')]
        token_vecs = [model.wv[t] for t in toks if t in model.wv]
        if token_vecs:
            vec = np.mean(token_vecs, axis=0)
        else:
            # ãƒ¢ãƒ‡ãƒ«ã« 'unk' ãŒãªã„å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            vec = model.wv['unk'] if 'unk' in model.wv else np.zeros(model.vector_size)
        vecs.append(vec)
    out_path = os.path.join(output_dir, 'vectors.npy')
    np.save(out_path, np.vstack(vecs))
    print(f"Saved node vectors to {out_path}")


def main():
    # 1) third_joint_graph.json ã‚’ã¾ã¨ã‚ã¦é›†ã‚ã‚‹
    files = []
    for gd in INPUT_GRAPH_DIRS:
        pattern = os.path.join(gd, 'case_*', 'third_joint_graph.json')
        files += glob.glob(pattern)
    files = sorted(files)
    if not files:
        print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {INPUT_GRAPH_DIRS}")
        return

    # 2) ã‚³ãƒ¼ãƒ‘ã‚¹ç”Ÿæˆ
    print("ğŸ“š Generating corpus â€¦")
    sentences = []
    for p in tqdm(files):
        g = load_graph(p)
        sentences += graph_to_sentences(g)
    sentences.append(["unk"])  # unk ãƒˆãƒ¼ã‚¯ãƒ³

    # 3) FastText å­¦ç¿’
    print("ğŸš€ Training FastText â€¦")
    ft = train_fasttext(sentences, DIM, EPOCH)
    ft.save(MODEL_PATH)
    print(f"âœ… model saved to {MODEL_PATH}")

    # 4) å„ã‚°ãƒ©ãƒ•ã”ã¨ã«ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜ï¼ˆå…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨åŒã˜å ´æ‰€ï¼‰
    print("ğŸ’¾ Saving node vectors â€¦")
    for p in tqdm(files):
        g = load_graph(p)
        # å…¥åŠ›ãƒ‘ã‚¹ã® case ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãã®ã¾ã¾å‡ºåŠ›å…ˆã«ä½¿ç”¨
        output_dir = os.path.dirname(p)
        save_node_vectors(g, ft, output_dir)

    print("âœ… All done.")

if __name__ == "__main__":
    main()
